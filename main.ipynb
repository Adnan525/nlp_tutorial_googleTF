{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3155d89-b148-4703-bf36-f86a99779561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80e900b8-19a9-4c7e-b804-c2bff0ab0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'i love my dog',\n",
    "    'I, love my cat',\n",
    "    'You love my dog!'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c23835-9c1c-42e3-9b97-ca4ef726cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_words define the number of words to keep\n",
    "# if we have a lot of words(more than num_words) it will keep most frequent num_words number of words\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2caeb749-e76b-4ce5-af86-74861019f379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "# each word willl have a numeric index\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cad94ddd-231e-4498-aeee-499662af184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to represent sentences using the word indexes\n",
    "# tokenizer is smart enough to know dog and dog! are same\n",
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f026abf1-f675-44f1-886a-d42bc967a1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
     ]
    }
   ],
   "source": [
    "# oov_token for out of vocabulary words of the tokenizer to contain all OOVs.\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d7c2471-ea34-47ce-b00f-8cdd41859f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "# now we will try to describe sentences using the tokens\n",
    "tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "767cf808-eda5-4c50-b3ba-75fd62536a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "# testing with new word\n",
    "test_sentence = [\"I love my car\"] # expects a list\n",
    "print(tokenizer.texts_to_sequences(test_sentence)) # missing \"car\", resulting sequence will have 3 words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4923faaa-6f2f-4efc-8c1a-f41de698c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOV token\n",
    "# the tokenizer will create a token for <OOV> and replace that with missing words, as a group\n",
    "tokenizer = Tokenizer(num_words=100, oov_token = \"<OOV>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb32115e-13af-4804-965a-c73d1e440530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '<OOV>', 2: 'my', 3: 'love', 4: 'dog', 5: 'i', 6: 'you', 7: 'cat', 8: 'do', 9: 'think', 10: 'is', 11: 'amazing'}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "print(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02477413-adee-4e47-b21c-4c8fa8ae9a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 3, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(test_sentence)) # car would be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "715f489a-6d4a-46ac-b525-816fb83bb6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 3, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences([\"I love my Mitsubishi\"])) # same value for unknown vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "400e962a-858d-4d92-a374-fb772db042d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ccb35a6-18b4-445d-85e0-2e6224460761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  5  3  2  4]\n",
      " [ 0  0  0  5  3  2  7]\n",
      " [ 0  0  0  6  3  2  4]\n",
      " [ 8  6  9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded_sequences = pad_sequences(sequences) # padding to longest sentence by default\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf2c71e-674f-445b-9a21-c81cb2c7aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding parameters\n",
    "# padding = \"post\" to add padding at the end\n",
    "# maxlen = 10 to set padded sentence length max to 10\n",
    "# truncating = \"post/pre\" remove post or pre if sentence length is greater than maxlen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f881d0-9ef9-4172-8867-7df86af360b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
